{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем один из файлов и посмотрим, что мы можем с ним сделать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Получив экземпляр первого издания романа «Алхимик», я испытал искреннюю радость. В течение многих лет читатели-энтузиасты в России пытались распространять книгу самостоятельно – размещали в Интернете, передавали друг другу в виде книжек-самоделок, изготавливали фотокопии текста, который вы сейчас держите в руках. Однако, несмотря на все старания, никак не удавалось, в силу разных обстоятельств, добиться грамотного продвижения «Алхимика» на книжный рынок.\\n\\nИ вот наконец книгу издали и занялись профессиональным распространением «Алхимика» на российском рынке.\\n\\nКлючевое понятие, которое лежит в основе повествования о путешествии пастуха Сантьяго,\\xa0– это понятие «Своя Судьба». Что же такое Своя Судьба? Это наше высшее предназначение, путь, уготованный нам Господом здесь, на Земле. Всякий раз, когда мы делаем что-то с радостью и удовольствием, это означает, что мы следуем Своей Судьбе. Однако не всем достает мужества идти по этому пути, добиваясь встречи со своей заветной мечтой.\\n\\nПочему же не у всех сбываются желания и мечты?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./test.txt\") as f:\n",
    "    example_text = f.read()\n",
    "\n",
    "example_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация текста - просто выделение оттуда отдельных слов. Обратите внимание, они не приводятся к одному регистру, знаки препинания не удаляются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Получив',\n",
       " 'экземпляр',\n",
       " 'первого',\n",
       " 'издания',\n",
       " 'романа',\n",
       " '«',\n",
       " 'Алхимик',\n",
       " '»',\n",
       " ',',\n",
       " 'я',\n",
       " 'испытал',\n",
       " 'искреннюю',\n",
       " 'радость',\n",
       " '.',\n",
       " 'В',\n",
       " 'течение',\n",
       " 'многих',\n",
       " 'лет',\n",
       " 'читатели-энтузиасты',\n",
       " 'в',\n",
       " 'России',\n",
       " 'пытались',\n",
       " 'распространять',\n",
       " 'книгу',\n",
       " 'самостоятельно',\n",
       " '–',\n",
       " 'размещали',\n",
       " 'в',\n",
       " 'Интернете',\n",
       " ',',\n",
       " 'передавали',\n",
       " 'друг',\n",
       " 'другу',\n",
       " 'в',\n",
       " 'виде',\n",
       " 'книжек-самоделок',\n",
       " ',',\n",
       " 'изготавливали',\n",
       " 'фотокопии',\n",
       " 'текста',\n",
       " ',',\n",
       " 'который',\n",
       " 'вы',\n",
       " 'сейчас',\n",
       " 'держите',\n",
       " 'в',\n",
       " 'руках',\n",
       " '.',\n",
       " 'Однако',\n",
       " ',',\n",
       " 'несмотря',\n",
       " 'на',\n",
       " 'все',\n",
       " 'старания',\n",
       " ',',\n",
       " 'никак',\n",
       " 'не',\n",
       " 'удавалось',\n",
       " ',',\n",
       " 'в',\n",
       " 'силу',\n",
       " 'разных',\n",
       " 'обстоятельств',\n",
       " ',',\n",
       " 'добиться',\n",
       " 'грамотного',\n",
       " 'продвижения',\n",
       " '«',\n",
       " 'Алхимика',\n",
       " '»',\n",
       " 'на',\n",
       " 'книжный',\n",
       " 'рынок',\n",
       " '.',\n",
       " 'И',\n",
       " 'вот',\n",
       " 'наконец',\n",
       " 'книгу',\n",
       " 'издали',\n",
       " 'и',\n",
       " 'занялись',\n",
       " 'профессиональным',\n",
       " 'распространением',\n",
       " '«',\n",
       " 'Алхимика',\n",
       " '»',\n",
       " 'на',\n",
       " 'российском',\n",
       " 'рынке',\n",
       " '.',\n",
       " 'Ключевое',\n",
       " 'понятие',\n",
       " ',',\n",
       " 'которое',\n",
       " 'лежит',\n",
       " 'в',\n",
       " 'основе',\n",
       " 'повествования',\n",
       " 'о',\n",
       " 'путешествии',\n",
       " 'пастуха',\n",
       " 'Сантьяго',\n",
       " ',',\n",
       " '–',\n",
       " 'это',\n",
       " 'понятие',\n",
       " '«',\n",
       " 'Своя',\n",
       " 'Судьба',\n",
       " '»',\n",
       " '.',\n",
       " 'Что',\n",
       " 'же',\n",
       " 'такое',\n",
       " 'Своя',\n",
       " 'Судьба',\n",
       " '?',\n",
       " 'Это',\n",
       " 'наше',\n",
       " 'высшее',\n",
       " 'предназначение',\n",
       " ',',\n",
       " 'путь',\n",
       " ',',\n",
       " 'уготованный',\n",
       " 'нам',\n",
       " 'Господом',\n",
       " 'здесь',\n",
       " ',',\n",
       " 'на',\n",
       " 'Земле',\n",
       " '.',\n",
       " 'Всякий',\n",
       " 'раз',\n",
       " ',',\n",
       " 'когда',\n",
       " 'мы',\n",
       " 'делаем',\n",
       " 'что-то',\n",
       " 'с',\n",
       " 'радостью',\n",
       " 'и',\n",
       " 'удовольствием',\n",
       " ',',\n",
       " 'это',\n",
       " 'означает',\n",
       " ',',\n",
       " 'что',\n",
       " 'мы',\n",
       " 'следуем',\n",
       " 'Своей',\n",
       " 'Судьбе',\n",
       " '.',\n",
       " 'Однако',\n",
       " 'не',\n",
       " 'всем',\n",
       " 'достает',\n",
       " 'мужества',\n",
       " 'идти',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'пути',\n",
       " ',',\n",
       " 'добиваясь',\n",
       " 'встречи',\n",
       " 'со',\n",
       " 'своей',\n",
       " 'заветной',\n",
       " 'мечтой',\n",
       " '.',\n",
       " 'Почему',\n",
       " 'же',\n",
       " 'не',\n",
       " 'у',\n",
       " 'всех',\n",
       " 'сбываются',\n",
       " 'желания',\n",
       " 'и',\n",
       " 'мечты',\n",
       " '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokens = nltk.word_tokenize(example_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся стеммером, чтобы привести слова к одной форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['получ',\n",
       " 'экземпляр',\n",
       " 'перв',\n",
       " 'издан',\n",
       " 'рома',\n",
       " '«алхимик»,',\n",
       " 'я',\n",
       " 'испыта',\n",
       " 'искрен',\n",
       " 'радость.',\n",
       " 'в',\n",
       " 'течен',\n",
       " 'мног',\n",
       " 'лет',\n",
       " 'читатели-энтузиаст',\n",
       " 'в',\n",
       " 'росс',\n",
       " 'пыта',\n",
       " 'распространя',\n",
       " 'книг',\n",
       " 'самостоятельн',\n",
       " '–',\n",
       " 'размеща',\n",
       " 'в',\n",
       " 'интернете,',\n",
       " 'передава',\n",
       " 'друг',\n",
       " 'друг',\n",
       " 'в',\n",
       " 'вид',\n",
       " 'книжек-самоделок,',\n",
       " 'изготавлива',\n",
       " 'фотокоп',\n",
       " 'текста,',\n",
       " 'котор',\n",
       " 'вы',\n",
       " 'сейчас',\n",
       " 'держ',\n",
       " 'в',\n",
       " 'руках.',\n",
       " 'однако,',\n",
       " 'несмотр',\n",
       " 'на',\n",
       " 'все',\n",
       " 'старания,',\n",
       " 'никак',\n",
       " 'не',\n",
       " 'удавалось,',\n",
       " 'в',\n",
       " 'сил',\n",
       " 'разн',\n",
       " 'обстоятельств,',\n",
       " 'доб',\n",
       " 'грамотн',\n",
       " 'продвижен',\n",
       " '«алхимика»',\n",
       " 'на',\n",
       " 'книжн',\n",
       " 'рынок.',\n",
       " 'и',\n",
       " 'вот',\n",
       " 'наконец',\n",
       " 'книг',\n",
       " 'изда',\n",
       " 'и',\n",
       " 'заня',\n",
       " 'профессиональн',\n",
       " 'распространен',\n",
       " '«алхимика»',\n",
       " 'на',\n",
       " 'российск',\n",
       " 'рынке.',\n",
       " 'ключев',\n",
       " 'понятие,',\n",
       " 'котор',\n",
       " 'леж',\n",
       " 'в',\n",
       " 'основ',\n",
       " 'повествован',\n",
       " 'о',\n",
       " 'путешеств',\n",
       " 'пастух',\n",
       " 'сантьяго,',\n",
       " '–',\n",
       " 'эт',\n",
       " 'понят',\n",
       " '«сво',\n",
       " 'судьба».',\n",
       " 'что',\n",
       " 'же',\n",
       " 'так',\n",
       " 'сво',\n",
       " 'судьба?',\n",
       " 'эт',\n",
       " 'наш',\n",
       " 'высш',\n",
       " 'предназначение,',\n",
       " 'путь,',\n",
       " 'уготова',\n",
       " 'нам',\n",
       " 'господ',\n",
       " 'здесь,',\n",
       " 'на',\n",
       " 'земле.',\n",
       " 'всяк',\n",
       " 'раз,',\n",
       " 'когд',\n",
       " 'мы',\n",
       " 'дела',\n",
       " 'что-т',\n",
       " 'с',\n",
       " 'радост',\n",
       " 'и',\n",
       " 'удовольствием,',\n",
       " 'эт',\n",
       " 'означает,',\n",
       " 'что',\n",
       " 'мы',\n",
       " 'следу',\n",
       " 'сво',\n",
       " 'судьбе.',\n",
       " 'однак',\n",
       " 'не',\n",
       " 'всем',\n",
       " 'доста',\n",
       " 'мужеств',\n",
       " 'идт',\n",
       " 'по',\n",
       " 'эт',\n",
       " 'пути,',\n",
       " 'добив',\n",
       " 'встреч',\n",
       " 'со',\n",
       " 'сво',\n",
       " 'заветн',\n",
       " 'мечтой.',\n",
       " 'поч',\n",
       " 'же',\n",
       " 'не',\n",
       " 'у',\n",
       " 'всех',\n",
       " 'сбыва',\n",
       " 'желан',\n",
       " 'и',\n",
       " 'мечты?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowballstemmer import RussianStemmer\n",
    "stemmer = RussianStemmer()\n",
    "stemmer.stemWords(example_text.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой вариант - вместо стеммера использовать лемматизатор - штуку, которая приводит слова к начальной форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['получать',\n",
       " ' ',\n",
       " 'экземпляр',\n",
       " ' ',\n",
       " 'первый',\n",
       " ' ',\n",
       " 'издание',\n",
       " ' ',\n",
       " 'роман',\n",
       " ' « ',\n",
       " 'алхимик',\n",
       " ' » , ',\n",
       " 'я',\n",
       " ' ',\n",
       " 'испытывать',\n",
       " ' ',\n",
       " 'искренний',\n",
       " ' ',\n",
       " 'радость',\n",
       " ' ',\n",
       " '. ',\n",
       " 'в',\n",
       " ' ',\n",
       " 'течение',\n",
       " ' ',\n",
       " 'многий',\n",
       " ' ',\n",
       " 'год',\n",
       " ' ',\n",
       " 'читатель',\n",
       " '-',\n",
       " 'энтузиаст',\n",
       " ' ',\n",
       " 'в',\n",
       " ' ',\n",
       " 'россия',\n",
       " ' ',\n",
       " 'пытаться',\n",
       " ' ',\n",
       " 'распространять',\n",
       " ' ',\n",
       " 'книга',\n",
       " ' ',\n",
       " 'самостоятельно',\n",
       " ' – ',\n",
       " 'размещать',\n",
       " ' ',\n",
       " 'в',\n",
       " ' ',\n",
       " 'интернет',\n",
       " ' , ',\n",
       " 'передавать',\n",
       " ' ',\n",
       " 'друг',\n",
       " ' ',\n",
       " 'друг',\n",
       " ' ',\n",
       " 'в',\n",
       " ' ',\n",
       " 'вид',\n",
       " ' ',\n",
       " 'книжка',\n",
       " '-',\n",
       " 'самоделка',\n",
       " ' , ',\n",
       " 'изготовлять',\n",
       " ' ',\n",
       " 'фотокопия',\n",
       " ' ',\n",
       " 'текст',\n",
       " ' , ',\n",
       " 'который',\n",
       " ' ',\n",
       " 'вы',\n",
       " ' ',\n",
       " 'сейчас',\n",
       " ' ',\n",
       " 'держать',\n",
       " ' ',\n",
       " 'в',\n",
       " ' ',\n",
       " 'рука',\n",
       " ' ',\n",
       " '. ',\n",
       " 'однако',\n",
       " ' , ',\n",
       " 'несмотря',\n",
       " ' ',\n",
       " 'на',\n",
       " ' ',\n",
       " 'весь',\n",
       " ' ',\n",
       " 'старание',\n",
       " ' , ',\n",
       " 'никак',\n",
       " ' ',\n",
       " 'не',\n",
       " ' ',\n",
       " 'удаваться',\n",
       " ' , ',\n",
       " 'в',\n",
       " ' ',\n",
       " 'сила',\n",
       " ' ',\n",
       " 'разный',\n",
       " ' ',\n",
       " 'обстоятельство',\n",
       " ' , ',\n",
       " 'добиваться',\n",
       " ' ',\n",
       " 'грамотный',\n",
       " ' ',\n",
       " 'продвижение',\n",
       " ' « ',\n",
       " 'алхимик',\n",
       " ' » ',\n",
       " 'на',\n",
       " ' ',\n",
       " 'книжный',\n",
       " ' ',\n",
       " 'рынок',\n",
       " ' ',\n",
       " '. ',\n",
       " 'и',\n",
       " ' ',\n",
       " 'вот',\n",
       " ' ',\n",
       " 'наконец',\n",
       " ' ',\n",
       " 'книга',\n",
       " ' ',\n",
       " 'издали',\n",
       " ' ',\n",
       " 'и',\n",
       " ' ',\n",
       " 'заниматься',\n",
       " ' ',\n",
       " 'профессиональный',\n",
       " ' ',\n",
       " 'распространение',\n",
       " ' « ',\n",
       " 'алхимик',\n",
       " ' » ',\n",
       " 'на',\n",
       " ' ',\n",
       " 'российский',\n",
       " ' ',\n",
       " 'рынок',\n",
       " ' ',\n",
       " '. ',\n",
       " 'ключевой',\n",
       " ' ',\n",
       " 'понятие',\n",
       " ' , ',\n",
       " 'который',\n",
       " ' ',\n",
       " 'лежать',\n",
       " ' ',\n",
       " 'в',\n",
       " ' ',\n",
       " 'основа',\n",
       " ' ',\n",
       " 'повествование',\n",
       " ' ',\n",
       " 'о',\n",
       " ' ',\n",
       " 'путешествие',\n",
       " ' ',\n",
       " 'пастух',\n",
       " ' ',\n",
       " 'сантьяго',\n",
       " ' , – ',\n",
       " 'это',\n",
       " ' ',\n",
       " 'понятие',\n",
       " ' « ',\n",
       " 'свой',\n",
       " ' ',\n",
       " 'судьба',\n",
       " ' » ',\n",
       " '. ',\n",
       " 'что',\n",
       " ' ',\n",
       " 'же',\n",
       " ' ',\n",
       " 'такой',\n",
       " ' ',\n",
       " 'свой',\n",
       " ' ',\n",
       " 'судьба',\n",
       " ' ',\n",
       " '? ',\n",
       " 'это',\n",
       " ' ',\n",
       " 'наш',\n",
       " ' ',\n",
       " 'высокий',\n",
       " ' ',\n",
       " 'предназначение',\n",
       " ' , ',\n",
       " 'путь',\n",
       " ' , ',\n",
       " 'уготовать',\n",
       " ' ',\n",
       " 'мы',\n",
       " ' ',\n",
       " 'господь',\n",
       " ' ',\n",
       " 'здесь',\n",
       " ' , ',\n",
       " 'на',\n",
       " ' ',\n",
       " 'земля',\n",
       " ' ',\n",
       " '. ',\n",
       " 'всякий',\n",
       " ' ',\n",
       " 'раз',\n",
       " ' , ',\n",
       " 'когда',\n",
       " ' ',\n",
       " 'мы',\n",
       " ' ',\n",
       " 'делать',\n",
       " ' ',\n",
       " 'что-то',\n",
       " ' ',\n",
       " 'с',\n",
       " ' ',\n",
       " 'радость',\n",
       " ' ',\n",
       " 'и',\n",
       " ' ',\n",
       " 'удовольствие',\n",
       " ' , ',\n",
       " 'это',\n",
       " ' ',\n",
       " 'означать',\n",
       " ' , ',\n",
       " 'что',\n",
       " ' ',\n",
       " 'мы',\n",
       " ' ',\n",
       " 'следуемый',\n",
       " ' ',\n",
       " 'свой',\n",
       " ' ',\n",
       " 'судьба',\n",
       " ' ',\n",
       " '. ',\n",
       " 'однако',\n",
       " ' ',\n",
       " 'не',\n",
       " ' ',\n",
       " 'все',\n",
       " ' ',\n",
       " 'доставать',\n",
       " ' ',\n",
       " 'мужество',\n",
       " ' ',\n",
       " 'идти',\n",
       " ' ',\n",
       " 'по',\n",
       " ' ',\n",
       " 'этот',\n",
       " ' ',\n",
       " 'путь',\n",
       " ' , ',\n",
       " 'добиваться',\n",
       " ' ',\n",
       " 'встреча',\n",
       " ' ',\n",
       " 'со',\n",
       " ' ',\n",
       " 'свой',\n",
       " ' ',\n",
       " 'заветный',\n",
       " ' ',\n",
       " 'мечта',\n",
       " ' ',\n",
       " '. ',\n",
       " 'почему',\n",
       " ' ',\n",
       " 'же',\n",
       " ' ',\n",
       " 'не',\n",
       " ' ',\n",
       " 'у',\n",
       " ' ',\n",
       " 'все',\n",
       " ' ',\n",
       " 'сбываться',\n",
       " ' ',\n",
       " 'желание',\n",
       " ' ',\n",
       " 'и',\n",
       " ' ',\n",
       " 'мечта',\n",
       " ' ',\n",
       " '?',\n",
       " '\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymystem3\n",
    "mystem = pymystem3.Mystem()\n",
    "lemm = mystem.lemmatize(' '.join(tokens[:200]))\n",
    "lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще провести лемматизацию слов можно с помощью морфологического анализатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['получить',\n",
       " 'экземпляр',\n",
       " 'первый',\n",
       " 'издание',\n",
       " 'роман',\n",
       " '«',\n",
       " 'алхимик',\n",
       " '»',\n",
       " ',',\n",
       " 'я',\n",
       " 'испытать',\n",
       " 'искренний',\n",
       " 'радость',\n",
       " '.',\n",
       " 'в',\n",
       " 'течение',\n",
       " 'многий',\n",
       " 'год',\n",
       " 'читатель-энтузиаст',\n",
       " 'в',\n",
       " 'россия',\n",
       " 'пытаться',\n",
       " 'распространять',\n",
       " 'книга',\n",
       " 'самостоятельно',\n",
       " '–',\n",
       " 'размещать',\n",
       " 'в',\n",
       " 'интернет',\n",
       " ',',\n",
       " 'передавать',\n",
       " 'друг',\n",
       " 'друг',\n",
       " 'в',\n",
       " 'вид',\n",
       " 'книжка-самоделка',\n",
       " ',',\n",
       " 'изготавливать',\n",
       " 'фотокопия',\n",
       " 'текст',\n",
       " ',',\n",
       " 'который',\n",
       " 'вы',\n",
       " 'сейчас',\n",
       " 'держать',\n",
       " 'в',\n",
       " 'рука',\n",
       " '.',\n",
       " 'однако',\n",
       " ',',\n",
       " 'несмотря',\n",
       " 'на',\n",
       " 'всё',\n",
       " 'старание',\n",
       " ',',\n",
       " 'никак',\n",
       " 'не',\n",
       " 'удаваться',\n",
       " ',',\n",
       " 'в',\n",
       " 'сила',\n",
       " 'разный',\n",
       " 'обстоятельство',\n",
       " ',',\n",
       " 'добиться',\n",
       " 'грамотный',\n",
       " 'продвижение',\n",
       " '«',\n",
       " 'алхимик',\n",
       " '»',\n",
       " 'на',\n",
       " 'книжный',\n",
       " 'рынок',\n",
       " '.',\n",
       " 'и',\n",
       " 'вот',\n",
       " 'наконец',\n",
       " 'книга',\n",
       " 'издали',\n",
       " 'и',\n",
       " 'заняться',\n",
       " 'профессиональный',\n",
       " 'распространение',\n",
       " '«',\n",
       " 'алхимик',\n",
       " '»',\n",
       " 'на',\n",
       " 'российский',\n",
       " 'рынок',\n",
       " '.',\n",
       " 'ключевой',\n",
       " 'понятие',\n",
       " ',',\n",
       " 'который',\n",
       " 'лежать',\n",
       " 'в',\n",
       " 'основа',\n",
       " 'повествование',\n",
       " 'о',\n",
       " 'путешествие',\n",
       " 'пастух',\n",
       " 'сантьяго',\n",
       " ',',\n",
       " '–',\n",
       " 'это',\n",
       " 'понятие',\n",
       " '«',\n",
       " 'свой',\n",
       " 'судьба',\n",
       " '»',\n",
       " '.',\n",
       " 'что',\n",
       " 'же',\n",
       " 'такой',\n",
       " 'свой',\n",
       " 'судьба',\n",
       " '?',\n",
       " 'это',\n",
       " 'наш',\n",
       " 'высокий',\n",
       " 'предназначение',\n",
       " ',',\n",
       " 'путь',\n",
       " ',',\n",
       " 'уготовать',\n",
       " 'мы',\n",
       " 'господь',\n",
       " 'здесь',\n",
       " ',',\n",
       " 'на',\n",
       " 'земля',\n",
       " '.',\n",
       " 'всякий',\n",
       " 'раз',\n",
       " ',',\n",
       " 'когда',\n",
       " 'мы',\n",
       " 'делать',\n",
       " 'что-то',\n",
       " 'с',\n",
       " 'радость',\n",
       " 'и',\n",
       " 'удовольствие',\n",
       " ',',\n",
       " 'это',\n",
       " 'означать',\n",
       " ',',\n",
       " 'что',\n",
       " 'мы',\n",
       " 'следуемый',\n",
       " 'свой',\n",
       " 'судьба',\n",
       " '.',\n",
       " 'однако',\n",
       " 'не',\n",
       " 'весь',\n",
       " 'доставать',\n",
       " 'мужество',\n",
       " 'идти',\n",
       " 'по',\n",
       " 'этот',\n",
       " 'путь',\n",
       " ',',\n",
       " 'добиваться',\n",
       " 'встреча',\n",
       " 'с',\n",
       " 'свой',\n",
       " 'заветный',\n",
       " 'мечта',\n",
       " '.',\n",
       " 'почему',\n",
       " 'же',\n",
       " 'не',\n",
       " 'у',\n",
       " 'весь',\n",
       " 'сбываться',\n",
       " 'желание',\n",
       " 'и',\n",
       " 'мечта',\n",
       " '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemm_pymorph = [morph.parse(s)[0].normal_form for s in tokens]\n",
    "lemm_pymorph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы снова собрать из этого, текст, просто сконкатенируем все получившиеся строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'получить экземпляр первый издание роман « алхимик » , я испытать искренний радость . в течение многий год читатель-энтузиаст в россия пытаться распространять книга самостоятельно – размещать в интернет , передавать друг друг в вид книжка-самоделка , изготавливать фотокопия текст , который вы сейчас держать в рука . однако , несмотря на всё старание , никак не удаваться , в сила разный обстоятельство , добиться грамотный продвижение « алхимик » на книжный рынок . и вот наконец книга издали и заняться профессиональный распространение « алхимик » на российский рынок . ключевой понятие , который лежать в основа повествование о путешествие пастух сантьяго , – это понятие « свой судьба » . что же такой свой судьба ? это наш высокий предназначение , путь , уготовать мы господь здесь , на земля . всякий раз , когда мы делать что-то с радость и удовольствие , это означать , что мы следуемый свой судьба . однако не весь доставать мужество идти по этот путь , добиваться встреча с свой заветный мечта . почему же не у весь сбываться желание и мечта ?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lemm_pymorph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование выборки сырых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернем наши предыдущие действия в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(input_text):\n",
    "    tokens = nltk.word_tokenize(input_text)\n",
    "    normed_tokens = [morph.parse(s)[0].normal_form for s in tokens]\n",
    "    # исключим также стоп-слова - всякие предлоги, союзы и т.п.\n",
    "    normed_tokens = [word for word in normed_tokens if word not in nltk.corpus.stopwords.words(\"russian\")]\n",
    "    # а также знаки препинания\n",
    "    normed_tokens = [word for word in normed_tokens if word not in punctuation]\n",
    "    return ' '.join(normed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу бинарной классификации текстов. Возьмем 2 текстa книг и попробуем подобрать такой алгоритм, который смог бы их отличать друг от друга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# подготовим пустой датафрейм\n",
    "df = pd.DataFrame(columns=['text', 'class'])\n",
    "\n",
    "# это папки, в которых лежат файлы с текстами\n",
    "dir0 = \"./little_guy\"\n",
    "dir1 = \"./punishment\"\n",
    "\n",
    "# считаем все наши тексты в датафрейм с указанием класса\n",
    "for filename in os.listdir(dir0):\n",
    "    with open(os.path.join(dir0, filename),encoding='utf8') as file:   \n",
    "        contents = lemmatize(file.read())\n",
    "    df = df.append(pd.Series({'text': contents, 'class': 0}), ignore_index=True)\n",
    "    \n",
    "# и для второй папки тоже\n",
    "for filename in os.listdir(dir1):\n",
    "    with open(os.path.join(dir1, filename),encoding='utf8') as file:\n",
    "        contents = lemmatize(file.read())\n",
    "    df = df.append(pd.Series({'text': contents, 'class': 1}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>маленький принц i шесть год книга название « п...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x близкий весь планета маленький принц астерои...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xi второй планета жить честолюбец — почитатель...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>хii следующий планета жить пьяница маленький п...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xiii четвёртый планета принадлежать деловой че...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>`` действительно разумихин недавно ещё хотеть ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>впоследствии раскольников случиться как-то узн...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>дверь отвориться крошечный щёлочка вострый нед...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>пролежать очень долго случаться просыпаться ми...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>`` обыск застать '' комната никто никто загляд...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text class\n",
       "0   маленький принц i шесть год книга название « п...     0\n",
       "1   x близкий весь планета маленький принц астерои...     0\n",
       "2   xi второй планета жить честолюбец — почитатель...     0\n",
       "3   хii следующий планета жить пьяница маленький п...     0\n",
       "4   xiii четвёртый планета принадлежать деловой че...     0\n",
       "..                                                ...   ...\n",
       "63  `` действительно разумихин недавно ещё хотеть ...     1\n",
       "64  впоследствии раскольников случиться как-то узн...     1\n",
       "65  дверь отвориться крошечный щёлочка вострый нед...     1\n",
       "66  пролежать очень долго случаться просыпаться ми...     1\n",
       "67  `` обыск застать '' комната никто никто загляд...     1\n",
       "\n",
       "[68 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем выборку на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['class'], test_size=0.3, stratify=df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words-эмбеддинг\n",
    "\n",
    "Понятное дело, работать с чистым текстом математические методы не умеют. Настало время получить эмбеддинги!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bof_vect = CountVectorizer()\n",
    "bof_vect.fit(np.hstack([X_train, X_test]))\n",
    "bof_train = bof_vect.transform(X_train)\n",
    "bof_test = bof_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на вид нашего нового признакового пространства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bof_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 12002)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bof_train.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF-эмбеддинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(np.hstack([X_train, X_test]))\n",
    "tfidf_train = tfidf_vect.transform(X_train)\n",
    "tfidf_test = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.01593842],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 12002)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec-эмбеддинг\n",
    "Поскольку w2v - это не sklearn'овский классификатор, он на выходе выдаст данные немного другого вида, и это надо будет учитывать в дальнейшей работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "X_train_w2v = X_train.apply(str.split)\n",
    "X_test_w2v = X_test.apply(str.split)\n",
    "w2v_vect = Word2Vec(np.hstack([X_train_w2v, X_test_w2v]), size=300, min_count=10, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57    [весь, вечер, десять, час, провести, разный, т...\n",
       "5     [xiv, пятый, планета, очень, занятный, оказать...\n",
       "53    [папироска, заговорить, порфирий, петрович, ко...\n",
       "9     [xviii, маленький, принц, пересечь, пустыня, н...\n",
       "35    [здоровый, здоровый, весело, крикнуть, навстре...\n",
       "54    [спешить, свидригайлов, мочь, надеяться, это, ...\n",
       "13    [xxi, тут-то, появиться, лис, —, здравствуй, —...\n",
       "16    [хxiv, миновать, неделя, пора, потерпеть, авар...\n",
       "27    [начало, июль, чрезвычайно, жаркое, время, веч...\n",
       "17    [xxv, —, человек, забираться, скорый, поезд, п...\n",
       "23    [vi, маленький, принц, понемногу, понять, такж...\n",
       "12    [хx, долго, идти, маленький, принц, песок, ска...\n",
       "26    [ix, понять, решить, странствовать, перелётный...\n",
       "11    [ii, жить, одиночество, поговорить, душа, шест...\n",
       "18    [xxvi, неподалёку, колодец, сохраниться, разва...\n",
       "34    [озаботить, серьёзный, проснуться, разумихин, ...\n",
       "31    [выйти, встать, заложить, крючок, дверь, развя...\n",
       "42    [главное, дело, последний, минута, никак, ожид...\n",
       "51    [лебезятников, иметь, вид, встревожить, софья,...\n",
       "46    [утро, последовать, роковой, пётр, петрович, о...\n",
       "56    [раскольник, пойти, вслед, это, вскричать, сви...\n",
       "39    [..., верить, мочь, верить, повторять, озадачи...\n",
       "45    [воспоминание, минута, раскольников, представл...\n",
       "0     [маленький, принц, i, шесть, год, книга, назва...\n",
       "6     [xv, шестой, планета, десять, большой, предыду...\n",
       "41    [восемь, час, оба, спешить, бакалеев, прийти, ...\n",
       "7     [xvi, итак, седьмой, планета, который, посетит...\n",
       "44    [утро, ровно, одиннадцать, час, раскольник, во...\n",
       "25    [viii, очень, скоро, хороший, узнать, цветок, ...\n",
       "38    [раскольник, привыкнуть, толпа, сказать, бежат...\n",
       "3     [хii, следующий, планета, жить, пьяница, мален...\n",
       "43    [раскольник, пойти, прямо, дом, канава, жить, ...\n",
       "60    [письмо, мать, измучить, относительно, главный...\n",
       "29    [зосим, высокий, жирный, человек, одутловатый,...\n",
       "48    [пётр, петрович, закричать, защитить, внушить,...\n",
       "8     [хvii, очень, хотеть, сострить, иной, поневоле...\n",
       "33    [раскольник, приподняться, сесть, диван, слабо...\n",
       "22    [v, каждый, день, узнавать, что-нибудь, новый,...\n",
       "49    [проснуться, день, поздно, тревожный, сон, сон...\n",
       "52    [раскольников, наступить, странный, время, точ...\n",
       "63    [``, действительно, разумихин, недавно, ещё, х...\n",
       "66    [пролежать, очень, долго, случаться, просыпать...\n",
       "55    [знать, мочь, рассказывать, начать, свидригайл...\n",
       "19    [xxvii, пройти, шесть, лет…, ещё, никто, это, ...\n",
       "14    [xxii, —, добрый, день, —, сказать, маленький,...\n",
       "40    [``, неужели, это, продолжение, сон, '', подум...\n",
       "61    [сибирь, берег, широкий, пустынный, река, стои...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем тексты произведений в вектора - возьмем сумму векторов всех слов, которые входят в прозу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-c817aeec8a46>:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vecs.append(w2v_vect[word])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57    [-0.010966268, -0.19642776, -0.15400949, 0.160...\n",
       "5     [0.0239139, -0.17394106, -0.21006007, 0.110992...\n",
       "53    [-0.01141335, -0.20530544, -0.1607142, 0.16761...\n",
       "9     [0.03885354, -0.17263985, -0.24157633, 0.09470...\n",
       "35    [-0.011503428, -0.21043126, -0.16428047, 0.171...\n",
       "54    [-0.010707934, -0.20916077, -0.16553909, 0.170...\n",
       "13    [0.03280443, -0.16863061, -0.2242568, 0.097972...\n",
       "16    [0.015357481, -0.18459915, -0.20034596, 0.1265...\n",
       "27    [-0.011412908, -0.19999173, -0.15624836, 0.163...\n",
       "17    [0.022438392, -0.17609249, -0.2082436, 0.11356...\n",
       "23    [0.00950568, -0.185874, -0.1892982, 0.13277794...\n",
       "12    [0.018591167, -0.17148457, -0.19661336, 0.1136...\n",
       "26    [0.017677208, -0.17779638, -0.19906552, 0.1191...\n",
       "11    [0.0106609315, -0.17407155, -0.18065007, 0.123...\n",
       "18    [0.012945451, -0.18495204, -0.19551872, 0.1293...\n",
       "34    [-0.011457428, -0.20925252, -0.16280709, 0.170...\n",
       "31    [-0.011799119, -0.20446031, -0.15965313, 0.167...\n",
       "42    [-0.011397906, -0.2102867, -0.16452438, 0.1710...\n",
       "51    [-0.012403926, -0.20872325, -0.16178618, 0.171...\n",
       "46    [-0.0115348585, -0.2052418, -0.15998901, 0.168...\n",
       "56    [-0.011499868, -0.21064202, -0.16486031, 0.172...\n",
       "39    [-0.01196477, -0.20628652, -0.16090004, 0.1689...\n",
       "45    [-0.01256584, -0.21289049, -0.1653959, 0.17461...\n",
       "0     [-0.0006770825, -0.17951337, -0.16062069, 0.13...\n",
       "6     [0.03326717, -0.1643027, -0.2214413, 0.0942699...\n",
       "41    [-0.010858485, -0.2129329, -0.16679871, 0.1730...\n",
       "7     [0.0010110573, -0.16080898, -0.14738825, 0.122...\n",
       "44    [-0.011401546, -0.20466802, -0.16011898, 0.167...\n",
       "25    [0.014654104, -0.17979896, -0.19487293, 0.1236...\n",
       "38    [-0.011646535, -0.20054844, -0.15641046, 0.164...\n",
       "3     [0.032309026, -0.1750409, -0.22983092, 0.10320...\n",
       "43    [-0.011782748, -0.20821151, -0.16284415, 0.170...\n",
       "60    [-0.011468234, -0.20283584, -0.15849197, 0.165...\n",
       "29    [-0.012264938, -0.2007987, -0.1553769, 0.16523...\n",
       "48    [-0.012041217, -0.20940138, -0.16301274, 0.171...\n",
       "8     [0.028402925, -0.1722172, -0.21776646, 0.10494...\n",
       "33    [-0.011566528, -0.20911683, -0.16249748, 0.170...\n",
       "22    [0.011545631, -0.16883329, -0.17809588, 0.1186...\n",
       "49    [-0.010498711, -0.2023294, -0.1596407, 0.16477...\n",
       "52    [-0.011687911, -0.20731607, -0.16214284, 0.169...\n",
       "63    [-0.010868657, -0.19541074, -0.15337893, 0.159...\n",
       "66    [-0.011795094, -0.20159663, -0.15708418, 0.165...\n",
       "55    [-0.010729988, -0.20423236, -0.16043593, 0.166...\n",
       "19    [0.0008361018, -0.1858318, -0.17040895, 0.1414...\n",
       "14    [0.046830747, -0.17386256, -0.2596624, 0.08863...\n",
       "40    [-0.010988989, -0.20765181, -0.16295838, 0.169...\n",
       "61    [-0.010286987, -0.19412541, -0.15241653, 0.158...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text2vec(text):\n",
    "    \"\"\"Усредняем векторы слов\"\"\"\n",
    "    vecs = []\n",
    "    for word in text:\n",
    "        try:\n",
    "            vecs.append(w2v_vect[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return np.sum(vecs, axis=0) / len(vecs)\n",
    "\n",
    "w2v_train = X_train_w2v.apply(text2vec)\n",
    "w2v_test = X_test_w2v.apply(text2vec)\n",
    "w2v_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация текстов\n",
    "\n",
    "Теперь у нас есть классическое признаковое описание каждого текста. Можем обучать классификаторы или придумать еще какую-то метрику.\n",
    "\n",
    "Давайте вычислим для каждого эмбеддинга по два суммарных вектора - для текстов Куприна и для текстов Коэльо. Для Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12002)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_guy_mean_bof = np.sum(bof_train[y_train == 0], axis=0)\n",
    "little_guy_mean_bof.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12002)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punishment_mean_bof = np.sum(bof_train[y_train == 1], axis=0)\n",
    "punishment_mean_bof.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12002)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_guy_mean_tfidf = np.sum(tfidf_train[y_train == 0], axis=0)\n",
    "little_guy_mean_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "punishment_mean_tfidf = np.sum(tfidf_train[y_train == 1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_guy_mean_w2v = np.sum(w2v_train[y_train == 0], axis=0)\n",
    "little_guy_mean_w2v.shape\n",
    "punishment_mean_w2v = np.sum(w2v_train[y_train == 1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на их вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 3, 1]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_guy_mean_bof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.01948601,\n",
       "         0.01593842]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punishment_mean_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7092787e-01, -3.3302915e+00, -3.8257163e+00,  2.1971252e+00,\n",
       "        9.8315662e-01, -1.7760329e+00,  2.7130105e+00,  1.6216923e+00,\n",
       "       -2.4020629e+00, -1.3693084e+00,  1.1908207e+00, -1.9335695e-01,\n",
       "        4.9617081e+00,  1.0360515e+00, -3.7746994e+00,  1.6549693e+00,\n",
       "        1.7334353e+00, -2.3942342e+00, -5.5022607e+00, -1.0688537e+00,\n",
       "        5.0580993e-02, -1.1330364e+00,  5.7979301e-02,  9.2752957e-01,\n",
       "       -2.4605331e+00,  2.5706995e+00, -9.1845101e-01, -2.0660360e-01,\n",
       "        7.4275637e-01,  1.1011337e-01, -4.7563696e+00,  3.9813747e+00,\n",
       "        8.3535177e-01, -2.3539448e-01,  3.0421585e-01, -3.6390715e+00,\n",
       "       -4.3951068e+00,  1.7787704e+00,  2.0127912e+00, -4.3204457e-01,\n",
       "       -1.9493415e+00, -1.3371359e+00, -1.4039849e-01, -1.7517266e+00,\n",
       "       -5.0576258e+00, -2.8715527e+00, -2.7435389e+00,  3.2809240e-01,\n",
       "       -3.6172409e+00,  5.6449542e+00,  4.9087489e-01,  9.6343809e-01,\n",
       "        2.7081091e+00,  3.9733531e+00,  6.2350434e-01, -2.1371117e+00,\n",
       "        1.9479972e+00,  2.1085863e+00,  3.1970894e+00, -4.9994817e+00,\n",
       "        8.1620741e-01,  1.5103235e+00,  1.4659524e-01, -1.9149255e+00,\n",
       "        1.0834144e+00, -4.8508307e-01, -1.6691585e+00,  6.2261252e+00,\n",
       "        3.0824605e-01,  2.6840024e-02,  4.0602694e+00, -9.8971462e-01,\n",
       "        3.2628067e+00, -1.2357718e+00, -1.0401385e+00,  3.0089917e+00,\n",
       "       -3.8707328e+00,  5.1488614e+00,  3.8654345e-01, -7.2873704e-02,\n",
       "       -3.4625990e+00,  1.4702863e+00, -3.1434292e-01,  2.5191147e+00,\n",
       "       -8.6399436e-01, -2.4763787e+00,  3.7566039e-01, -4.0690866e+00,\n",
       "        1.9357109e+00,  1.3107687e+00,  5.9013338e+00,  1.9313523e+00,\n",
       "        2.0522034e+00,  1.4287336e+00, -3.0565646e+00,  1.7572632e+00,\n",
       "       -3.9742916e+00,  1.6842391e+00, -1.3628364e+00, -2.9354062e+00,\n",
       "       -5.9003007e-01,  1.9468246e+00,  6.9257605e-01, -1.1316934e+00,\n",
       "       -1.5081495e-01, -2.7842480e-01, -1.1247288e+00,  3.8247435e+00,\n",
       "        6.0424404e+00,  2.4089572e+00,  4.3050008e+00, -3.9973131e-01,\n",
       "        1.4246485e+00, -1.8723457e+00, -1.6120119e+00,  3.6224792e+00,\n",
       "       -4.2501564e+00, -2.5657308e-01,  7.2412914e-01, -4.0281507e-01,\n",
       "       -1.9309481e+00,  1.9391687e+00,  2.0884330e+00, -3.4996805e-01,\n",
       "        5.3459632e-01,  2.1749640e+00,  1.0707833e+00,  7.4398923e-01,\n",
       "        2.4768836e+00,  1.4188385e+00, -4.5874176e+00, -1.8260822e+00,\n",
       "        1.0412934e+00,  4.4243116e+00,  5.2483970e-01,  3.0383792e+00,\n",
       "        2.6778495e-01,  2.6566250e+00,  5.3468609e-01,  3.3128281e+00,\n",
       "        3.8861442e+00,  2.3861740e+00,  1.1071600e+00,  2.3187258e+00,\n",
       "        9.1967955e-03,  1.5558518e+00,  1.5474895e+00, -2.6555023e+00,\n",
       "        5.6324458e+00,  6.4338231e+00, -1.1081758e+00, -1.3654484e+00,\n",
       "       -2.8195174e+00,  1.3782833e+00,  3.5540324e-01,  1.2943805e+00,\n",
       "        5.4204661e-01,  4.4533402e-01, -1.6331568e+00, -3.0724845e+00,\n",
       "       -2.9204106e+00,  4.7059546e+00,  5.0501032e+00,  1.1977689e+00,\n",
       "        8.0719113e-01,  3.3357842e+00,  3.2422791e+00,  1.8520275e-01,\n",
       "        7.3742085e+00,  2.8709021e-01,  1.7108572e+00, -4.8700225e-01,\n",
       "       -2.4903336e+00, -2.7928729e+00,  5.0101566e+00, -1.4550358e+00,\n",
       "       -5.6936293e+00,  7.0708327e+00,  4.8295660e+00,  8.8466579e-01,\n",
       "       -3.2680454e+00,  2.9627328e+00, -2.1977632e+00, -6.1910009e-01,\n",
       "        2.0709996e+00,  6.4929919e+00, -3.5210047e+00, -1.1066461e+00,\n",
       "        5.3886118e+00,  1.4094048e+00, -1.1343082e+00,  1.6669182e+00,\n",
       "        3.8482192e+00,  6.5159683e+00,  1.5811402e+00,  2.1927528e+00,\n",
       "       -4.6295449e-01,  3.1019807e+00,  7.4646583e+00,  4.2298589e+00,\n",
       "        4.3104539e+00,  6.4755449e+00,  3.2663383e+00,  1.7569798e+00,\n",
       "        3.1639850e+00,  3.1792037e+00,  6.6461980e-01,  5.0294679e-01,\n",
       "       -1.1068567e+00,  4.0348639e+00, -1.4082414e+00, -3.9137604e+00,\n",
       "        1.8207546e+00,  4.2966781e+00, -3.6260326e+00,  1.5441064e+00,\n",
       "       -6.2585139e+00,  2.1924057e+00, -5.8380957e+00, -2.5618598e+00,\n",
       "        1.9918662e-02,  4.0008512e+00,  3.2099726e+00, -7.7860266e-01,\n",
       "        8.9695638e-01,  2.6154068e+00,  1.9175948e+00,  7.4950653e-01,\n",
       "        6.7264062e-01, -3.4382641e-01,  3.2139747e+00,  4.0782251e+00,\n",
       "       -3.2052867e+00, -1.0718160e+00,  2.0589693e+00,  1.7621368e-04,\n",
       "       -1.0323566e+00,  4.3688269e+00,  1.5380113e+00, -3.4099569e+00,\n",
       "        4.0428084e-01,  2.0803740e+00,  2.8710455e-01, -2.5163593e+00,\n",
       "       -1.3238031e+00, -1.3491764e+00, -4.6213799e+00,  2.6704028e+00,\n",
       "        2.8501880e+00, -1.2922710e+00, -6.2291038e-01, -5.4797559e+00,\n",
       "       -1.9810015e+00, -9.8828542e-01, -1.6158347e+00, -6.5743130e-01,\n",
       "       -1.4847989e+00, -4.0182689e-01, -4.0817504e+00, -3.5730379e+00,\n",
       "       -3.8306634e+00,  6.6855917e+00, -4.4387274e+00, -3.2365100e+00,\n",
       "       -2.1699650e+00,  1.4679917e+00, -6.0640551e-02,  3.0924523e+00,\n",
       "       -3.1206901e+00,  1.4400563e+00,  7.6538259e-01, -5.4587191e-01,\n",
       "       -3.5135536e+00,  1.9636037e+00,  4.4690747e+00, -3.4809580e+00,\n",
       "       -1.3372905e+00, -2.1787856e+00, -2.2807052e+00, -1.1921191e+00,\n",
       "        2.7426827e+00, -1.3880349e+00, -1.4615607e+00, -8.5036713e-01,\n",
       "       -3.9643259e+00, -3.0048111e-01,  5.0398059e+00,  4.3641176e+00,\n",
       "        8.3580464e-01,  4.6233811e+00,  8.5113132e-01, -3.5462978e+00,\n",
       "        1.5172718e-01,  4.2337790e+00, -5.0081724e-01,  1.0546879e+00,\n",
       "       -2.3288378e-01,  4.9565032e-01, -3.4693909e+00, -1.8036529e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_guy_mean_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь построим датафреймы с результатами классификации тестовых текстов. Будем считать, что текст относится к тому произведению, с которым его косинусное расстояние больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False  True False  True  True  True  True False  True  True\n",
      " False False False  True  True  True  True False  True]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>little_guy</th>\n",
       "      <th>punishment</th>\n",
       "      <th>predict</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.347993</td>\n",
       "      <td>0.646712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.533236</td>\n",
       "      <td>0.234699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.382146</td>\n",
       "      <td>0.708942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.484417</td>\n",
       "      <td>0.192325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.387089</td>\n",
       "      <td>0.624223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.705058</td>\n",
       "      <td>0.439003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.522856</td>\n",
       "      <td>0.162715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.471950</td>\n",
       "      <td>0.200783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.588303</td>\n",
       "      <td>0.316510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.604912</td>\n",
       "      <td>0.888734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.572460</td>\n",
       "      <td>0.225324</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.616911</td>\n",
       "      <td>0.332559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.439595</td>\n",
       "      <td>0.575302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.427376</td>\n",
       "      <td>0.560337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.590969</td>\n",
       "      <td>0.823630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.541917</td>\n",
       "      <td>0.223553</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.535108</td>\n",
       "      <td>0.157590</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.528255</td>\n",
       "      <td>0.240016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.504568</td>\n",
       "      <td>0.170118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.396781</td>\n",
       "      <td>0.569009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.532076</td>\n",
       "      <td>0.212325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    little_guy  punishment  predict  class\n",
       "0     0.347993    0.646712      0.0    0.0\n",
       "1     0.533236    0.234699      1.0    1.0\n",
       "2     0.382146    0.708942      0.0    0.0\n",
       "3     0.484417    0.192325      1.0    1.0\n",
       "4     0.387089    0.624223      0.0    0.0\n",
       "5     0.705058    0.439003      1.0    1.0\n",
       "6     0.522856    0.162715      1.0    1.0\n",
       "7     0.471950    0.200783      1.0    1.0\n",
       "8     0.588303    0.316510      1.0    1.0\n",
       "9     0.604912    0.888734      0.0    0.0\n",
       "10    0.572460    0.225324      1.0    1.0\n",
       "11    0.616911    0.332559      1.0    1.0\n",
       "12    0.439595    0.575302      0.0    0.0\n",
       "13    0.427376    0.560337      0.0    0.0\n",
       "14    0.590969    0.823630      0.0    0.0\n",
       "15    0.541917    0.223553      1.0    1.0\n",
       "16    0.535108    0.157590      1.0    1.0\n",
       "17    0.528255    0.240016      1.0    1.0\n",
       "18    0.504568    0.170118      1.0    1.0\n",
       "19    0.396781    0.569009      0.0    0.0\n",
       "20    0.532076    0.212325      1.0    1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "bof_little_guy = np.apply_along_axis(cosine, 1, bof_test.toarray(), v=little_guy_mean_bof)\n",
    "bof_punishment = np.apply_along_axis(cosine, 1, bof_test.toarray(), v=punishment_mean_bof)\n",
    "\n",
    "print(np.maximum(bof_little_guy,bof_punishment)==bof_little_guy)\n",
    "\n",
    "bof_results = pd.DataFrame([\n",
    "    bof_little_guy,\n",
    "    bof_punishment,\n",
    "    np.maximum(bof_little_guy, bof_punishment)==bof_little_guy,   \n",
    "    y_test\n",
    "], index=[\"little_guy\", \"punishment\", \"predict\", \"class\"]).T.astype(np.float)\n",
    "bof_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посчитаем accuracy для предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(bof_results['predict'], bof_results['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>little_guy</th>\n",
       "      <th>punishment</th>\n",
       "      <th>predict</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.640919</td>\n",
       "      <td>0.844060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.778922</td>\n",
       "      <td>0.444013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.683053</td>\n",
       "      <td>0.888736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.742740</td>\n",
       "      <td>0.431957</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.608607</td>\n",
       "      <td>0.818161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.876633</td>\n",
       "      <td>0.609593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.767429</td>\n",
       "      <td>0.378562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.729716</td>\n",
       "      <td>0.395510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.791935</td>\n",
       "      <td>0.532689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.778348</td>\n",
       "      <td>0.941509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.803985</td>\n",
       "      <td>0.435784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.818683</td>\n",
       "      <td>0.547491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.621076</td>\n",
       "      <td>0.811592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.693826</td>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.777539</td>\n",
       "      <td>0.921521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.795778</td>\n",
       "      <td>0.471945</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.771694</td>\n",
       "      <td>0.376506</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.758545</td>\n",
       "      <td>0.468622</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.751462</td>\n",
       "      <td>0.331002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.663101</td>\n",
       "      <td>0.815769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.776281</td>\n",
       "      <td>0.430090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    little_guy  punishment  predict  class\n",
       "0     0.640919    0.844060      0.0    0.0\n",
       "1     0.778922    0.444013      1.0    1.0\n",
       "2     0.683053    0.888736      0.0    0.0\n",
       "3     0.742740    0.431957      1.0    1.0\n",
       "4     0.608607    0.818161      0.0    0.0\n",
       "5     0.876633    0.609593      1.0    1.0\n",
       "6     0.767429    0.378562      1.0    1.0\n",
       "7     0.729716    0.395510      1.0    1.0\n",
       "8     0.791935    0.532689      1.0    1.0\n",
       "9     0.778348    0.941509      0.0    0.0\n",
       "10    0.803985    0.435784      1.0    1.0\n",
       "11    0.818683    0.547491      1.0    1.0\n",
       "12    0.621076    0.811592      0.0    0.0\n",
       "13    0.693826    0.801926      0.0    0.0\n",
       "14    0.777539    0.921521      0.0    0.0\n",
       "15    0.795778    0.471945      1.0    1.0\n",
       "16    0.771694    0.376506      1.0    1.0\n",
       "17    0.758545    0.468622      1.0    1.0\n",
       "18    0.751462    0.331002      1.0    1.0\n",
       "19    0.663101    0.815769      0.0    0.0\n",
       "20    0.776281    0.430090      1.0    1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_little_guy = np.apply_along_axis(cosine, 1, tfidf_test.toarray(), v=little_guy_mean_tfidf)\n",
    "tfidf_punishment = np.apply_along_axis(cosine, 1, tfidf_test.toarray(), v=punishment_mean_tfidf)\n",
    "\n",
    "tfidf_results = pd.DataFrame([\n",
    "    tfidf_little_guy,\n",
    "    tfidf_punishment,\n",
    "    np.maximum(tfidf_little_guy, tfidf_punishment) == tfidf_little_guy,\n",
    "    y_test\n",
    "], index=[\"little_guy\", \"punishment\", \"predict\", \"class\"]).T.astype(np.float)\n",
    "tfidf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(tfidf_results['predict'], tfidf_results['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (21,) (300,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-2a5625d81e3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v_little_guy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlittle_guy_mean_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mw2v_punishment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpunishment_mean_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m w2v_results = pd.DataFrame([\n\u001b[0;32m      5\u001b[0m     \u001b[0mw2v_little_guy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         ) from None\n\u001b[1;32m--> 379\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcosine\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[1;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;31m#   or 'reflective correlation'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[1;34m(u, v, w, centered)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mumu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m     \u001b[0muv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m     \u001b[0muu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[0mvv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (21,) (300,) "
     ]
    }
   ],
   "source": [
    "w2v_little_guy = np.apply_along_axis(cosine, 0, w2v_test, v=little_guy_mean_w2v)\n",
    "w2v_punishment = np.apply_along_axis(cosine, 0, w2v_test, v=punishment_mean_w2v)\n",
    "\n",
    "w2v_results = pd.DataFrame([\n",
    "    w2v_little_guy,\n",
    "    w2v_punishment,\n",
    "    np.maximum(w2v_little_guy, w2v_punishment) == w2v_little_guy,\n",
    "    y_test\n",
    "], index=[\"little_guy\", \"punishment\", \"predict\", \"class\"]).T.astype(np.float)\n",
    "w2v_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(w2v_results['predict'], w2v_results['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier().fit(bof_train.toarray(), y_train.tolist()).score(bof_test.toarray(), y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().fit(tfidf_train.toarray(), y_train.tolist()).score(tfidf_test.toarray(), y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-265b825985c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             )\n\u001b[1;32m--> 303\u001b[1;33m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[0;32m    304\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    796\u001b[0m         \"\"\"\n\u001b[1;32m--> 797\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python38\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "RandomForestClassifier().fit(w2v_train.T, y_train.tolist()).score(w2v_test.T, y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
